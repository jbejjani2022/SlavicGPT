{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev notebook for data exploration and model baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary exploration of Russian literature text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data file\n",
    "data_path = 'data/tiny-russian-lit/very_clean_tiny_russian_lit.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in for inspection\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset at data/tiny-russian-lit/very_clean_tiny_russian_lit.txt is 34824628 characters\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dataset at {data_path} is {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters of the dataset:\n",
      " Михаил Лермонтов\n",
      "  \n",
      "\n",
      "Выхожу один я на дорогу;\n",
      "Сквозь туман кремнистый путь блестит;\n",
      "Ночь тиха. Пустыня внемлет богу,\n",
      "И звезда с звездою говорит.\n",
      "\n",
      "В небесах торжественно и чудно!\n",
      "Спит земля в сиянье голубом...\n",
      "Что же мне так больно и так трудно?\n",
      "Жду ль чего? жалею ли о чем?\n",
      "\n",
      "Уж не жду от жизни ничего я,\n",
      "И не жаль мне прошлого ничуть;\n",
      "Я ищу свободы и покоя!\n",
      "Я б хотел забыться и заснуть!\n",
      "\n",
      "Но не тем холодным сном могилы...\n",
      "Я б желал навеки так заснуть,\n",
      "Чтоб в груди дремали жизни силы,\n",
      "Чтоб, дыша, вздымалась тихо грудь;\n",
      "\n",
      "Чтоб всю ночь, весь день мой слух лелея,\n",
      "Про любовь мне сладкий голос пел,\n",
      "Надо мной чтоб, вечно зеленея,\n",
      "Темный дуб склонялся и шумел.\n",
      "Михаил Лермонтов\n",
      "ВАЛЕРИК\n",
      "Я к вам пишу случайно; право,\n",
      "Не знаю как и для чего.\n",
      "Я потерял уж это право.\n",
      "И что скажу вам? — ничего!\n",
      "Что помню вас? — но, боже правый,\n",
      "Вы это знаете давно;\n",
      "И вам, конечно, все равно.\n",
      "И знать вам также нету нужды,\n",
      "Где я? что я? в какой глуши?\n",
      "Душою мы друг другу чужды,\n",
      "Да вряд ли есть родство души.\n",
      "Страницы прошл\n"
     ]
    }
   ],
   "source": [
    "print(f'First 1000 characters of the dataset:\\n {text[:1000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocabulary: \n",
      " !&,-.:;?i ̀́ЁІЉАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёєі–—’\n",
      "Vocabulary size: 87\n"
     ]
    }
   ],
   "source": [
    "# find the unique characters that occur in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab = ''.join(chars)\n",
    "vocab_size = len(chars)\n",
    "print(f'Text vocabulary: {vocab}\\nVocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to be able to tokenize our input - convert raw string text into a sequence of integers according to our vocabulary of possible elements.\n",
    "\n",
    "For a character-level language model, each character in our vocabulary gets tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple character-level tokenizer: a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: convert string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: convert list of integers to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(string):\n",
    "    print(f\"The string '{string}' has the encoding {encode(string)}\")\n",
    "    print(decode(encode(string)) == string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "char = decode([1])\n",
    "utf8_encoded = char.encode('utf-8')\n",
    "print(utf8_encoded.hex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string ' ' has the encoding [1]\n",
      "True\n",
      "The string '\n",
      "' has the encoding [0]\n",
      "True\n",
      "The string 'и' has the encoding [57]\n",
      "True\n",
      "The string 'Мой дядя самых честных правил' has the encoding [29, 63, 58, 1, 53, 80, 53, 80, 1, 66, 49, 61, 76, 70, 1, 72, 54, 66, 67, 62, 76, 70, 1, 64, 65, 49, 51, 57, 60]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "verify(' ')\n",
    "verify('\\n')\n",
    "verify('и')\n",
    "verify('Мой дядя самых честных правил')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data tensor has shape torch.Size([34824628]) and type torch.int64\n",
      "First 1000 elements of data tensor:\n",
      " tensor([29, 57, 70, 49, 57, 60,  1, 28, 54, 65, 61, 63, 62, 67, 63, 51,  0,  1,\n",
      "         1,  0,  0, 19, 76, 70, 63, 55, 68,  1, 63, 53, 57, 62,  1, 80,  1, 62,\n",
      "        49,  1, 53, 63, 65, 63, 52, 68,  8,  0, 34, 59, 51, 63, 56, 77,  1, 67,\n",
      "        68, 61, 49, 62,  1, 59, 65, 54, 61, 62, 57, 66, 67, 76, 58,  1, 64, 68,\n",
      "        67, 77,  1, 50, 60, 54, 66, 67, 57, 67,  8,  0, 30, 63, 72, 77,  1, 67,\n",
      "        57, 70, 49,  6,  1, 32, 68, 66, 67, 76, 62, 80,  1, 51, 62, 54, 61, 60,\n",
      "        54, 67,  1, 50, 63, 52, 68,  4,  0, 25,  1, 56, 51, 54, 56, 53, 49,  1,\n",
      "        66,  1, 56, 51, 54, 56, 53, 63, 79,  1, 52, 63, 51, 63, 65, 57, 67,  6,\n",
      "         0,  0, 19,  1, 62, 54, 50, 54, 66, 49, 70,  1, 67, 63, 65, 55, 54, 66,\n",
      "        67, 51, 54, 62, 62, 63,  1, 57,  1, 72, 68, 53, 62, 63,  2,  0, 34, 64,\n",
      "        57, 67,  1, 56, 54, 61, 60, 80,  1, 51,  1, 66, 57, 80, 62, 77, 54,  1,\n",
      "        52, 63, 60, 68, 50, 63, 61,  6,  6,  6,  0, 40, 67, 63,  1, 55, 54,  1,\n",
      "        61, 62, 54,  1, 67, 49, 59,  1, 50, 63, 60, 77, 62, 63,  1, 57,  1, 67,\n",
      "        49, 59,  1, 67, 65, 68, 53, 62, 63,  9,  0, 23, 53, 68,  1, 60, 77,  1,\n",
      "        72, 54, 52, 63,  9,  1, 55, 49, 60, 54, 79,  1, 60, 57,  1, 63,  1, 72,\n",
      "        54, 61,  9,  0,  0, 36, 55,  1, 62, 54,  1, 55, 53, 68,  1, 63, 67,  1,\n",
      "        55, 57, 56, 62, 57,  1, 62, 57, 72, 54, 52, 63,  1, 80,  4,  0, 25,  1,\n",
      "        62, 54,  1, 55, 49, 60, 77,  1, 61, 62, 54,  1, 64, 65, 63, 73, 60, 63,\n",
      "        52, 63,  1, 62, 57, 72, 68, 67, 77,  8,  0, 48,  1, 57, 74, 68,  1, 66,\n",
      "        51, 63, 50, 63, 53, 76,  1, 57,  1, 64, 63, 59, 63, 80,  2,  0, 48,  1,\n",
      "        50,  1, 70, 63, 67, 54, 60,  1, 56, 49, 50, 76, 67, 77, 66, 80,  1, 57,\n",
      "         1, 56, 49, 66, 62, 68, 67, 77,  2,  0,  0, 30, 63,  1, 62, 54,  1, 67,\n",
      "        54, 61,  1, 70, 63, 60, 63, 53, 62, 76, 61,  1, 66, 62, 63, 61,  1, 61,\n",
      "        63, 52, 57, 60, 76,  6,  6,  6,  0, 48,  1, 50,  1, 55, 54, 60, 49, 60,\n",
      "         1, 62, 49, 51, 54, 59, 57,  1, 67, 49, 59,  1, 56, 49, 66, 62, 68, 67,\n",
      "        77,  4,  0, 40, 67, 63, 50,  1, 51,  1, 52, 65, 68, 53, 57,  1, 53, 65,\n",
      "        54, 61, 49, 60, 57,  1, 55, 57, 56, 62, 57,  1, 66, 57, 60, 76,  4,  0,\n",
      "        40, 67, 63, 50,  4,  1, 53, 76, 73, 49,  4,  1, 51, 56, 53, 76, 61, 49,\n",
      "        60, 49, 66, 77,  1, 67, 57, 70, 63,  1, 52, 65, 68, 53, 77,  8,  0,  0,\n",
      "        40, 67, 63, 50,  1, 51, 66, 79,  1, 62, 63, 72, 77,  4,  1, 51, 54, 66,\n",
      "        77,  1, 53, 54, 62, 77,  1, 61, 63, 58,  1, 66, 60, 68, 70,  1, 60, 54,\n",
      "        60, 54, 80,  4,  0, 32, 65, 63,  1, 60, 79, 50, 63, 51, 77,  1, 61, 62,\n",
      "        54,  1, 66, 60, 49, 53, 59, 57, 58,  1, 52, 63, 60, 63, 66,  1, 64, 54,\n",
      "        60,  4,  0, 30, 49, 53, 63,  1, 61, 62, 63, 58,  1, 72, 67, 63, 50,  4,\n",
      "         1, 51, 54, 72, 62, 63,  1, 56, 54, 60, 54, 62, 54, 80,  4,  0, 35, 54,\n",
      "        61, 62, 76, 58,  1, 53, 68, 50,  1, 66, 59, 60, 63, 62, 80, 60, 66, 80,\n",
      "         1, 57,  1, 73, 68, 61, 54, 60,  6,  0, 29, 57, 70, 49, 57, 60,  1, 28,\n",
      "        54, 65, 61, 63, 62, 67, 63, 51,  0, 19, 17, 28, 22, 33, 25, 27,  0, 48,\n",
      "         1, 59,  1, 51, 49, 61,  1, 64, 57, 73, 68,  1, 66, 60, 68, 72, 49, 58,\n",
      "        62, 63,  8,  1, 64, 65, 49, 51, 63,  4,  0, 30, 54,  1, 56, 62, 49, 79,\n",
      "         1, 59, 49, 59,  1, 57,  1, 53, 60, 80,  1, 72, 54, 52, 63,  6,  0, 48,\n",
      "         1, 64, 63, 67, 54, 65, 80, 60,  1, 68, 55,  1, 78, 67, 63,  1, 64, 65,\n",
      "        49, 51, 63,  6,  0, 25,  1, 72, 67, 63,  1, 66, 59, 49, 55, 68,  1, 51,\n",
      "        49, 61,  9,  1, 85,  1, 62, 57, 72, 54, 52, 63,  2,  0, 40, 67, 63,  1,\n",
      "        64, 63, 61, 62, 79,  1, 51, 49, 66,  9,  1, 85,  1, 62, 63,  4,  1, 50,\n",
      "        63, 55, 54,  1, 64, 65, 49, 51, 76, 58,  4,  0, 19, 76,  1, 78, 67, 63,\n",
      "         1, 56, 62, 49, 54, 67, 54,  1, 53, 49, 51, 62, 63,  8,  0, 25,  1, 51,\n",
      "        49, 61,  4,  1, 59, 63, 62, 54, 72, 62, 63,  4,  1, 51, 66, 54,  1, 65,\n",
      "        49, 51, 62, 63,  6,  0, 25,  1, 56, 62, 49, 67, 77,  1, 51, 49, 61,  1,\n",
      "        67, 49, 59, 55, 54,  1, 62, 54, 67, 68,  1, 62, 68, 55, 53, 76,  4,  0,\n",
      "        20, 53, 54,  1, 80,  9,  1, 72, 67, 63,  1, 80,  9,  1, 51,  1, 59, 49,\n",
      "        59, 63, 58,  1, 52, 60, 68, 73, 57,  9,  0, 21, 68, 73, 63, 79,  1, 61,\n",
      "        76,  1, 53, 65, 68, 52,  1, 53, 65, 68, 52, 68,  1, 72, 68, 55, 53, 76,\n",
      "         4,  0, 21, 49,  1, 51, 65, 80, 53,  1, 60, 57,  1, 54, 66, 67, 77,  1,\n",
      "        65, 63, 53, 66, 67, 51, 63,  1, 53, 68, 73, 57,  6,  0, 34, 67, 65, 49,\n",
      "        62, 57, 71, 76,  1, 64, 65, 63, 73, 60])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store in a tensor\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'Input data tensor has shape {data.shape} and type {data.dtype}')\n",
    "print(f'First 1000 elements of data tensor:\\n {data[:1000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation sets to test for overfitting\n",
    "split = 0.8\n",
    "n = int(split*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block size, or context length, is the max length of any individual chunk of text that the transformer is trained on. A chunk of text of length `block_size + 1` has `block_size` individual training examples. This also means that the size of the input to the transformer at sampling time will never exceed `block_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First block of the training data, + 1 character: tensor([29, 57, 70, 49, 57, 60,  1, 28, 54])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "first_block = train_data[:block_size + 1]\n",
    "print(f'First block of the training data, + 1 character: {first_block}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given block of text with length block_size + 1, we will train the transformer on each sequence/target pair from length 1 to block_size (where target is character immediately following the last character in the sequence). This is done so that the transformer is 'used' to predicting the next token given contexts of length as small as 1 and as large as block_size. This is important at sampling time, where the transformer has to begin generating targets from a context of potentially less than block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples/sequences in first block of data\n",
      "1/8: When input is, tensor([29]) target is 57\n",
      "2/8: When input is, tensor([29, 57]) target is 70\n",
      "3/8: When input is, tensor([29, 57, 70]) target is 49\n",
      "4/8: When input is, tensor([29, 57, 70, 49]) target is 57\n",
      "5/8: When input is, tensor([29, 57, 70, 49, 57]) target is 60\n",
      "6/8: When input is, tensor([29, 57, 70, 49, 57, 60]) target is 1\n",
      "7/8: When input is, tensor([29, 57, 70, 49, 57, 60,  1]) target is 28\n",
      "8/8: When input is, tensor([29, 57, 70, 49, 57, 60,  1, 28]) target is 54\n"
     ]
    }
   ],
   "source": [
    "print(f'Training examples/sequences in first block of data')\n",
    "for i in range(1, block_size + 1):\n",
    "    print(f'{i}/{block_size}: When input is, {first_block[:i]} target is {first_block[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[70,  6,  1, 31, 67, 65, 80, 53],\n",
      "        [49,  1, 72, 54, 60, 63, 51, 54],\n",
      "        [66, 54, 50, 54,  1, 57, 61, 57],\n",
      "        [65, 62, 63, 54,  4,  1, 55, 53]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 6,  1, 31, 67, 65, 80, 53,  1],\n",
      "        [ 1, 72, 54, 60, 63, 51, 54, 59],\n",
      "        [54, 50, 54,  1, 57, 61, 57,  1],\n",
      "        [62, 63, 54,  4,  1, 55, 53, 54]])\n",
      "----------\n",
      "Batch 1/4\n",
      "When input is [70], target is 6\n",
      "When input is [70, 6], target is 1\n",
      "When input is [70, 6, 1], target is 31\n",
      "When input is [70, 6, 1, 31], target is 67\n",
      "When input is [70, 6, 1, 31, 67], target is 65\n",
      "When input is [70, 6, 1, 31, 67, 65], target is 80\n",
      "When input is [70, 6, 1, 31, 67, 65, 80], target is 53\n",
      "When input is [70, 6, 1, 31, 67, 65, 80, 53], target is 1\n",
      "Batch 2/4\n",
      "When input is [49], target is 1\n",
      "When input is [49, 1], target is 72\n",
      "When input is [49, 1, 72], target is 54\n",
      "When input is [49, 1, 72, 54], target is 60\n",
      "When input is [49, 1, 72, 54, 60], target is 63\n",
      "When input is [49, 1, 72, 54, 60, 63], target is 51\n",
      "When input is [49, 1, 72, 54, 60, 63, 51], target is 54\n",
      "When input is [49, 1, 72, 54, 60, 63, 51, 54], target is 59\n",
      "Batch 3/4\n",
      "When input is [66], target is 54\n",
      "When input is [66, 54], target is 50\n",
      "When input is [66, 54, 50], target is 54\n",
      "When input is [66, 54, 50, 54], target is 1\n",
      "When input is [66, 54, 50, 54, 1], target is 57\n",
      "When input is [66, 54, 50, 54, 1, 57], target is 61\n",
      "When input is [66, 54, 50, 54, 1, 57, 61], target is 57\n",
      "When input is [66, 54, 50, 54, 1, 57, 61, 57], target is 1\n",
      "Batch 4/4\n",
      "When input is [65], target is 62\n",
      "When input is [65, 62], target is 63\n",
      "When input is [65, 62, 63], target is 54\n",
      "When input is [65, 62, 63, 54], target is 4\n",
      "When input is [65, 62, 63, 54, 4], target is 1\n",
      "When input is [65, 62, 63, 54, 4, 1], target is 55\n",
      "When input is [65, 62, 63, 54, 4, 1, 55], target is 53\n",
      "When input is [65, 62, 63, 54, 4, 1, 55, 53], target is 54\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "batch_size = 4  # the number of independent sequences that we will process in parallel\n",
    "block_size = 8  # maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a batch of data consisting of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # generate batch_size random offsets in the interval [0, len(data) - batch_size)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print(f'Batch {b + 1}/{batch_size}')\n",
    "    for t in range(block_size): # time/position dimension\n",
    "        context = xb[b, : t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When input is {context.tolist()}, target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the simplest language model is a bi-gram with character-based tokens. Given a single character, it predicts the next character in the sequence. I now implement a bi-gram as a baseline for our Russian text generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits (input to softmax) for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers (B = # batches, T = # timesteps/block size)\n",
    "        # we are essentially predicting the next character based on the embedding of a single token\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C) : batch, time, channels\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits since cross_entropy expects (B, C, T) inputs\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)  # equivalently, targets.view(-1)\n",
    "\n",
    "            # negative log likelihood loss - calculates quality of our logits with respect to the true targets\n",
    "            # a 'good' logit will have a high value in the target dimension and low values in other dimensions\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # the bigram only uses the last char as the context\n",
    "        # we pass in the full context here as practice for generation using transformer\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)  # calls the forward function\n",
    "            # retrieve only final timestep\n",
    "            logits = logits[:, -1, :] # (B, T, C) -> (B, C)\n",
    "            # apply softmax to get probability distribution\n",
    "            dist = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(dist, num_samples=1) # (B, 1)\n",
    "            # append new sample to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 87])\n",
      "tensor(4.8334, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)  # 4 batches, 8 timesteps, vocab_size channels\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "\n",
      "Sample: \n",
      "ПхчМнЭХ–г!,цЙщЫ,рАНвщiсЛжрЭтЗБИзняН—́’зЩЯщБги;ыРуШжмгЕЙСыТПУг–ПФЦырщп́ФЕпЫЧпо т!Их-фУЬюш–лрёФъЛшШi:пМш’̀Гб—мСЁМвєГчВ\n",
      "Х̀Ю&ТЁбєЙлкiыубц\n",
      "ЛИЭЫтущп́оНд:р:мвущфНдеєоЪЬЁІЙёЯэбИ?кІЙнЉк’є РЙ!Ип’м-оф&ЙЦкЭ’,Хп́каыцУуйНБ\n",
      "Н’лЯШшРОящ\n",
      "СБШОЮЁфу;—––тЯэ?ЁеЛзІыцфшАзП;є\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "def sample(context, new_tokens=100):\n",
    "    print(f'Context: {decode(context[0].tolist())}')\n",
    "    sample = model.generate(context, new_tokens)\n",
    "    text = decode(sample[0].tolist())\n",
    "    print(f'Sample: {text}')\n",
    "\n",
    "\n",
    "# as the model's starting context for sampling, let's provide a newline character\n",
    "blank_context = torch.tensor([encode('\\n')])\n",
    "sample(blank_context, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above sampled text is gibberish. Let's train the model so it can produce something that looks more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical lr setting is 3e-4, but for small models we can use a much higher lr\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/10000: loss=4.9720001220703125\n",
      "Step 10000/10000: loss=2.5729243755340576\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_steps = 10000\n",
    "for step in range(num_steps):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step == 0 or step == num_steps - 1:\n",
    "        print(f'Step {step + 1}/{num_steps}: loss={loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimization, let's see if we can sample something more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "\n",
      "Sample: \n",
      "Го-ибедора зл скро, но? чуковши  побннонизмиведныеебой некаяЫЫЕй бебля — воси вистспрося, еле, дак ся, жеюц вероя бы жныстомуб бугле Пра. узал этудукахой Вые дил космяме, мегажапошесиемонаюм х а в ногого Г?\n",
      "Ф-сстракат ушарать ро гомыцей я уде былетат\n"
     ]
    }
   ],
   "source": [
    "sample(blank_context, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look more like Russian text, but it is still pretty much gibberish. This is because the bigram predicts the next token only by looking at the last token in the context window. I.e. our 'context' is just one token. We're not learning any complex language patterns this way. With a transformer, we can enable the tokens to 'talk' to each other over longer ranges to learn more complex dependencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
