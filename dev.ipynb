{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev notebook for data exploration, model baselines, and experimentation with self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary exploration of Russian literature text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data file\n",
    "data_path = 'data/tiny-russian-lit/very_clean_tiny_russian_lit.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in for inspection\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset at data/tiny-russian-lit/very_clean_tiny_russian_lit.txt is 34824628 characters\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dataset at {data_path} is {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters of the dataset:\n",
      " Михаил Лермонтов\n",
      "  \n",
      "\n",
      "Выхожу один я на дорогу;\n",
      "Сквозь туман кремнистый путь блестит;\n",
      "Ночь тиха. Пустыня внемлет богу,\n",
      "И звезда с звездою говорит.\n",
      "\n",
      "В небесах торжественно и чудно!\n",
      "Спит земля в сиянье голубом...\n",
      "Что же мне так больно и так трудно?\n",
      "Жду ль чего? жалею ли о чем?\n",
      "\n",
      "Уж не жду от жизни ничего я,\n",
      "И не жаль мне прошлого ничуть;\n",
      "Я ищу свободы и покоя!\n",
      "Я б хотел забыться и заснуть!\n",
      "\n",
      "Но не тем холодным сном могилы...\n",
      "Я б желал навеки так заснуть,\n",
      "Чтоб в груди дремали жизни силы,\n",
      "Чтоб, дыша, вздымалась тихо грудь;\n",
      "\n",
      "Чтоб всю ночь, весь день мой слух лелея,\n",
      "Про любовь мне сладкий голос пел,\n",
      "Надо мной чтоб, вечно зеленея,\n",
      "Темный дуб склонялся и шумел.\n",
      "Михаил Лермонтов\n",
      "ВАЛЕРИК\n",
      "Я к вам пишу случайно; право,\n",
      "Не знаю как и для чего.\n",
      "Я потерял уж это право.\n",
      "И что скажу вам? — ничего!\n",
      "Что помню вас? — но, боже правый,\n",
      "Вы это знаете давно;\n",
      "И вам, конечно, все равно.\n",
      "И знать вам также нету нужды,\n",
      "Где я? что я? в какой глуши?\n",
      "Душою мы друг другу чужды,\n",
      "Да вряд ли есть родство души.\n",
      "Страницы прошл\n"
     ]
    }
   ],
   "source": [
    "print(f'First 1000 characters of the dataset:\\n {text[:1000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocabulary: \n",
      " !&,-.:;?i ̀́ЁІЉАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёєі–—’\n",
      "Vocabulary size: 87\n"
     ]
    }
   ],
   "source": [
    "# find the unique characters that occur in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab = ''.join(chars)\n",
    "vocab_size = len(chars)\n",
    "print(f'Text vocabulary: {vocab}\\nVocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to be able to tokenize our input - convert raw string text into a sequence of integers according to our vocabulary of possible elements.\n",
    "\n",
    "For a character-level language model, each character in our vocabulary gets tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple character-level tokenizer: a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: convert string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: convert list of integers to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(string):\n",
    "    print(f\"The string '{string}' has the encoding {encode(string)}\")\n",
    "    print(decode(encode(string)) == string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "char = decode([1])\n",
    "utf8_encoded = char.encode('utf-8')\n",
    "print(utf8_encoded.hex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string ' ' has the encoding [1]\n",
      "True\n",
      "The string '\n",
      "' has the encoding [0]\n",
      "True\n",
      "The string 'и' has the encoding [57]\n",
      "True\n",
      "The string 'Мой дядя самых честных правил' has the encoding [29, 63, 58, 1, 53, 80, 53, 80, 1, 66, 49, 61, 76, 70, 1, 72, 54, 66, 67, 62, 76, 70, 1, 64, 65, 49, 51, 57, 60]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "verify(' ')\n",
    "verify('\\n')\n",
    "verify('и')\n",
    "verify('Мой дядя самых честных правил')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data tensor has shape torch.Size([34824628]) and type torch.int64\n",
      "First 1000 elements of data tensor:\n",
      " tensor([29, 57, 70, 49, 57, 60,  1, 28, 54, 65, 61, 63, 62, 67, 63, 51,  0,  1,\n",
      "         1,  0,  0, 19, 76, 70, 63, 55, 68,  1, 63, 53, 57, 62,  1, 80,  1, 62,\n",
      "        49,  1, 53, 63, 65, 63, 52, 68,  8,  0, 34, 59, 51, 63, 56, 77,  1, 67,\n",
      "        68, 61, 49, 62,  1, 59, 65, 54, 61, 62, 57, 66, 67, 76, 58,  1, 64, 68,\n",
      "        67, 77,  1, 50, 60, 54, 66, 67, 57, 67,  8,  0, 30, 63, 72, 77,  1, 67,\n",
      "        57, 70, 49,  6,  1, 32, 68, 66, 67, 76, 62, 80,  1, 51, 62, 54, 61, 60,\n",
      "        54, 67,  1, 50, 63, 52, 68,  4,  0, 25,  1, 56, 51, 54, 56, 53, 49,  1,\n",
      "        66,  1, 56, 51, 54, 56, 53, 63, 79,  1, 52, 63, 51, 63, 65, 57, 67,  6,\n",
      "         0,  0, 19,  1, 62, 54, 50, 54, 66, 49, 70,  1, 67, 63, 65, 55, 54, 66,\n",
      "        67, 51, 54, 62, 62, 63,  1, 57,  1, 72, 68, 53, 62, 63,  2,  0, 34, 64,\n",
      "        57, 67,  1, 56, 54, 61, 60, 80,  1, 51,  1, 66, 57, 80, 62, 77, 54,  1,\n",
      "        52, 63, 60, 68, 50, 63, 61,  6,  6,  6,  0, 40, 67, 63,  1, 55, 54,  1,\n",
      "        61, 62, 54,  1, 67, 49, 59,  1, 50, 63, 60, 77, 62, 63,  1, 57,  1, 67,\n",
      "        49, 59,  1, 67, 65, 68, 53, 62, 63,  9,  0, 23, 53, 68,  1, 60, 77,  1,\n",
      "        72, 54, 52, 63,  9,  1, 55, 49, 60, 54, 79,  1, 60, 57,  1, 63,  1, 72,\n",
      "        54, 61,  9,  0,  0, 36, 55,  1, 62, 54,  1, 55, 53, 68,  1, 63, 67,  1,\n",
      "        55, 57, 56, 62, 57,  1, 62, 57, 72, 54, 52, 63,  1, 80,  4,  0, 25,  1,\n",
      "        62, 54,  1, 55, 49, 60, 77,  1, 61, 62, 54,  1, 64, 65, 63, 73, 60, 63,\n",
      "        52, 63,  1, 62, 57, 72, 68, 67, 77,  8,  0, 48,  1, 57, 74, 68,  1, 66,\n",
      "        51, 63, 50, 63, 53, 76,  1, 57,  1, 64, 63, 59, 63, 80,  2,  0, 48,  1,\n",
      "        50,  1, 70, 63, 67, 54, 60,  1, 56, 49, 50, 76, 67, 77, 66, 80,  1, 57,\n",
      "         1, 56, 49, 66, 62, 68, 67, 77,  2,  0,  0, 30, 63,  1, 62, 54,  1, 67,\n",
      "        54, 61,  1, 70, 63, 60, 63, 53, 62, 76, 61,  1, 66, 62, 63, 61,  1, 61,\n",
      "        63, 52, 57, 60, 76,  6,  6,  6,  0, 48,  1, 50,  1, 55, 54, 60, 49, 60,\n",
      "         1, 62, 49, 51, 54, 59, 57,  1, 67, 49, 59,  1, 56, 49, 66, 62, 68, 67,\n",
      "        77,  4,  0, 40, 67, 63, 50,  1, 51,  1, 52, 65, 68, 53, 57,  1, 53, 65,\n",
      "        54, 61, 49, 60, 57,  1, 55, 57, 56, 62, 57,  1, 66, 57, 60, 76,  4,  0,\n",
      "        40, 67, 63, 50,  4,  1, 53, 76, 73, 49,  4,  1, 51, 56, 53, 76, 61, 49,\n",
      "        60, 49, 66, 77,  1, 67, 57, 70, 63,  1, 52, 65, 68, 53, 77,  8,  0,  0,\n",
      "        40, 67, 63, 50,  1, 51, 66, 79,  1, 62, 63, 72, 77,  4,  1, 51, 54, 66,\n",
      "        77,  1, 53, 54, 62, 77,  1, 61, 63, 58,  1, 66, 60, 68, 70,  1, 60, 54,\n",
      "        60, 54, 80,  4,  0, 32, 65, 63,  1, 60, 79, 50, 63, 51, 77,  1, 61, 62,\n",
      "        54,  1, 66, 60, 49, 53, 59, 57, 58,  1, 52, 63, 60, 63, 66,  1, 64, 54,\n",
      "        60,  4,  0, 30, 49, 53, 63,  1, 61, 62, 63, 58,  1, 72, 67, 63, 50,  4,\n",
      "         1, 51, 54, 72, 62, 63,  1, 56, 54, 60, 54, 62, 54, 80,  4,  0, 35, 54,\n",
      "        61, 62, 76, 58,  1, 53, 68, 50,  1, 66, 59, 60, 63, 62, 80, 60, 66, 80,\n",
      "         1, 57,  1, 73, 68, 61, 54, 60,  6,  0, 29, 57, 70, 49, 57, 60,  1, 28,\n",
      "        54, 65, 61, 63, 62, 67, 63, 51,  0, 19, 17, 28, 22, 33, 25, 27,  0, 48,\n",
      "         1, 59,  1, 51, 49, 61,  1, 64, 57, 73, 68,  1, 66, 60, 68, 72, 49, 58,\n",
      "        62, 63,  8,  1, 64, 65, 49, 51, 63,  4,  0, 30, 54,  1, 56, 62, 49, 79,\n",
      "         1, 59, 49, 59,  1, 57,  1, 53, 60, 80,  1, 72, 54, 52, 63,  6,  0, 48,\n",
      "         1, 64, 63, 67, 54, 65, 80, 60,  1, 68, 55,  1, 78, 67, 63,  1, 64, 65,\n",
      "        49, 51, 63,  6,  0, 25,  1, 72, 67, 63,  1, 66, 59, 49, 55, 68,  1, 51,\n",
      "        49, 61,  9,  1, 85,  1, 62, 57, 72, 54, 52, 63,  2,  0, 40, 67, 63,  1,\n",
      "        64, 63, 61, 62, 79,  1, 51, 49, 66,  9,  1, 85,  1, 62, 63,  4,  1, 50,\n",
      "        63, 55, 54,  1, 64, 65, 49, 51, 76, 58,  4,  0, 19, 76,  1, 78, 67, 63,\n",
      "         1, 56, 62, 49, 54, 67, 54,  1, 53, 49, 51, 62, 63,  8,  0, 25,  1, 51,\n",
      "        49, 61,  4,  1, 59, 63, 62, 54, 72, 62, 63,  4,  1, 51, 66, 54,  1, 65,\n",
      "        49, 51, 62, 63,  6,  0, 25,  1, 56, 62, 49, 67, 77,  1, 51, 49, 61,  1,\n",
      "        67, 49, 59, 55, 54,  1, 62, 54, 67, 68,  1, 62, 68, 55, 53, 76,  4,  0,\n",
      "        20, 53, 54,  1, 80,  9,  1, 72, 67, 63,  1, 80,  9,  1, 51,  1, 59, 49,\n",
      "        59, 63, 58,  1, 52, 60, 68, 73, 57,  9,  0, 21, 68, 73, 63, 79,  1, 61,\n",
      "        76,  1, 53, 65, 68, 52,  1, 53, 65, 68, 52, 68,  1, 72, 68, 55, 53, 76,\n",
      "         4,  0, 21, 49,  1, 51, 65, 80, 53,  1, 60, 57,  1, 54, 66, 67, 77,  1,\n",
      "        65, 63, 53, 66, 67, 51, 63,  1, 53, 68, 73, 57,  6,  0, 34, 67, 65, 49,\n",
      "        62, 57, 71, 76,  1, 64, 65, 63, 73, 60])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store in a tensor\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'Input data tensor has shape {data.shape} and type {data.dtype}')\n",
    "print(f'First 1000 elements of data tensor:\\n {data[:1000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation sets to test for overfitting\n",
    "split = 0.8\n",
    "n = int(split*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block size, or context length, is the max length of any individual chunk of text that the transformer is trained on. A chunk of text of length `block_size + 1` has `block_size` individual training examples. This also means that the size of the input to the transformer at sampling time will never exceed `block_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First block of the training data, + 1 character: tensor([29, 57, 70, 49, 57, 60,  1, 28, 54])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "first_block = train_data[:block_size + 1]\n",
    "print(f'First block of the training data, + 1 character: {first_block}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given block of text with length `block_size + 1`, we will train the transformer on each sequence/target pair from length 1 to block_size (where target is character immediately following the last character in the sequence). This is done so that the transformer is 'used' to predicting the next token given contexts of length as small as 1 and as large as block_size. This is important at sampling time, where the transformer has to begin generating targets from a context of potentially less than block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples/sequences in first block of data\n",
      "1/8: When input is, tensor([29]) target is 57\n",
      "2/8: When input is, tensor([29, 57]) target is 70\n",
      "3/8: When input is, tensor([29, 57, 70]) target is 49\n",
      "4/8: When input is, tensor([29, 57, 70, 49]) target is 57\n",
      "5/8: When input is, tensor([29, 57, 70, 49, 57]) target is 60\n",
      "6/8: When input is, tensor([29, 57, 70, 49, 57, 60]) target is 1\n",
      "7/8: When input is, tensor([29, 57, 70, 49, 57, 60,  1]) target is 28\n",
      "8/8: When input is, tensor([29, 57, 70, 49, 57, 60,  1, 28]) target is 54\n"
     ]
    }
   ],
   "source": [
    "print(f'Training examples/sequences in first block of data')\n",
    "for i in range(1, block_size + 1):\n",
    "    print(f'{i}/{block_size}: When input is, {first_block[:i]} target is {first_block[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[70,  6,  1, 31, 67, 65, 80, 53],\n",
      "        [49,  1, 72, 54, 60, 63, 51, 54],\n",
      "        [66, 54, 50, 54,  1, 57, 61, 57],\n",
      "        [65, 62, 63, 54,  4,  1, 55, 53]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 6,  1, 31, 67, 65, 80, 53,  1],\n",
      "        [ 1, 72, 54, 60, 63, 51, 54, 59],\n",
      "        [54, 50, 54,  1, 57, 61, 57,  1],\n",
      "        [62, 63, 54,  4,  1, 55, 53, 54]])\n",
      "----------\n",
      "Batch 1/4\n",
      "When input is [70], target is 6\n",
      "When input is [70, 6], target is 1\n",
      "When input is [70, 6, 1], target is 31\n",
      "When input is [70, 6, 1, 31], target is 67\n",
      "When input is [70, 6, 1, 31, 67], target is 65\n",
      "When input is [70, 6, 1, 31, 67, 65], target is 80\n",
      "When input is [70, 6, 1, 31, 67, 65, 80], target is 53\n",
      "When input is [70, 6, 1, 31, 67, 65, 80, 53], target is 1\n",
      "Batch 2/4\n",
      "When input is [49], target is 1\n",
      "When input is [49, 1], target is 72\n",
      "When input is [49, 1, 72], target is 54\n",
      "When input is [49, 1, 72, 54], target is 60\n",
      "When input is [49, 1, 72, 54, 60], target is 63\n",
      "When input is [49, 1, 72, 54, 60, 63], target is 51\n",
      "When input is [49, 1, 72, 54, 60, 63, 51], target is 54\n",
      "When input is [49, 1, 72, 54, 60, 63, 51, 54], target is 59\n",
      "Batch 3/4\n",
      "When input is [66], target is 54\n",
      "When input is [66, 54], target is 50\n",
      "When input is [66, 54, 50], target is 54\n",
      "When input is [66, 54, 50, 54], target is 1\n",
      "When input is [66, 54, 50, 54, 1], target is 57\n",
      "When input is [66, 54, 50, 54, 1, 57], target is 61\n",
      "When input is [66, 54, 50, 54, 1, 57, 61], target is 57\n",
      "When input is [66, 54, 50, 54, 1, 57, 61, 57], target is 1\n",
      "Batch 4/4\n",
      "When input is [65], target is 62\n",
      "When input is [65, 62], target is 63\n",
      "When input is [65, 62, 63], target is 54\n",
      "When input is [65, 62, 63, 54], target is 4\n",
      "When input is [65, 62, 63, 54, 4], target is 1\n",
      "When input is [65, 62, 63, 54, 4, 1], target is 55\n",
      "When input is [65, 62, 63, 54, 4, 1, 55], target is 53\n",
      "When input is [65, 62, 63, 54, 4, 1, 55, 53], target is 54\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "batch_size = 4  # the number of independent sequences that we will process in parallel\n",
    "block_size = 8  # maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a batch of data consisting of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # generate batch_size random offsets in the interval [0, len(data) - batch_size)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print(f'Batch {b + 1}/{batch_size}')\n",
    "    for t in range(block_size): # time/position dimension\n",
    "        context = xb[b, : t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When input is {context.tolist()}, target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the simplest language model is a bi-gram with character-based tokens. Given a single character, it predicts the next character in the sequence. I now implement a bi-gram as a baseline for our Russian text generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits (input to softmax) for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers (B = # batches, T = # timesteps/block size)\n",
    "        # we are essentially predicting the next character based on the embedding of a single token\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C) : batch, time, channels\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits since cross_entropy expects (B, C, T) inputs\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)  # equivalently, targets.view(-1)\n",
    "\n",
    "            # negative log likelihood loss - calculates quality of our logits with respect to the true targets\n",
    "            # a 'good' logit will have a high value in the target dimension and low values in other dimensions\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # the bigram only uses the last char as the context\n",
    "        # we pass in the full context here as practice for generation using transformer\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)  # calls the forward function\n",
    "            # retrieve only final timestep\n",
    "            logits = logits[:, -1, :] # (B, T, C) -> (B, C)\n",
    "            # apply softmax to get probability distribution\n",
    "            dist = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(dist, num_samples=1) # (B, 1)\n",
    "            # append new sample to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 87])\n",
      "tensor(4.8334, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)  # 4 batches, 8 timesteps, vocab_size channels\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "\n",
      "Sample: \n",
      "ПхчМнЭХ–г!,цЙщЫ,рАНвщiсЛжрЭтЗБИзняН—́’зЩЯщБги;ыРуШжмгЕЙСыТПУг–ПФЦырщп́ФЕпЫЧпо т!Их-фУЬюш–лрёФъЛшШi:пМш’̀Гб—мСЁМвєГчВ\n",
      "Х̀Ю&ТЁбєЙлкiыубц\n",
      "ЛИЭЫтущп́оНд:р:мвущфНдеєоЪЬЁІЙёЯэбИ?кІЙнЉк’є РЙ!Ип’м-оф&ЙЦкЭ’,Хп́каыцУуйНБ\n",
      "Н’лЯШшРОящ\n",
      "СБШОЮЁфу;—––тЯэ?ЁеЛзІыцфшАзП;є\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "def sample(context, new_tokens=100):\n",
    "    print(f'Context: {decode(context[0].tolist())}')\n",
    "    sample = model.generate(context, new_tokens)\n",
    "    text = decode(sample[0].tolist())\n",
    "    print(f'Sample: {text}')\n",
    "\n",
    "\n",
    "# as the model's starting context for sampling, let's provide a newline character\n",
    "blank_context = torch.tensor([encode('\\n')])\n",
    "sample(blank_context, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above sampled text is gibberish. Let's train the model so it can produce something that looks more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical lr setting is 3e-4, but for small models we can use a much higher lr\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/10000: loss=4.9720001220703125\n",
      "Step 10000/10000: loss=2.5729243755340576\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_steps = 10000\n",
    "for step in range(num_steps):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step == 0 or step == num_steps - 1:\n",
    "        print(f'Step {step + 1}/{num_steps}: loss={loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimization, let's see if we can sample something more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "\n",
      "Sample: \n",
      "О—  -тх Вск убрдцелящушь уря,  кивобрия инет, стоси иск ветоешать кРОният, всгосска  М.\n",
      "у. в Бетокудпе чене в о серо тыхусви.Са выки сяЫ руга ожал зврегше нарого- споцостстошичимот скусяЖГц вс нени фи онослорь этолс вел. Аако прой  н Ктс ниц стренене\n"
     ]
    }
   ],
   "source": [
    "sample(blank_context, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is starting to look more like Russian text, but it is still pretty much gibberish. This is because the bigram predicts the next token only by looking at the last token in the context window. I.e. our 'context' is just one token. We're not learning any complex language patterns this way. With a transformer, we can enable the tokens to 'talk' to each other over longer ranges to learn more complex dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "# here's a toy example\n",
    "B, T, C = 4, 8, 2 # batch, time, channels\n",
    "x = torch.randn(B, T, C)  # C is the dim of features we track at each timestep in the sequence\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want information to flow from start of sequence to current timestep; we don't want current token to communicate with future tokens in the sequence. At inference time, we don't know what these will be yet. In short, we want tokens to communicate only with past tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very simple form of this communication is a \"BOW,\" or Bag of Words, approach: for each token at the `t`th timestep, we get the mean of the feature vectors of the timesteps up to and including `t`. This is very lossy because it does not capture any spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xbow[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t, C)\n",
    "        xbow[b, t] = torch.mean(xprev, 0)  # C\n",
    "        \n",
    "xbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0766,  0.3599],\n",
       "        [-0.7820,  0.0715],\n",
       "        [ 0.6648, -0.2868],\n",
       "        [ 1.6206, -1.5967],\n",
       "        [-0.0517, -0.3060],\n",
       "        [ 0.2485, -0.2226],\n",
       "        [ 0.9132,  0.2043],\n",
       "        [ 0.5740,  0.4163]])"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0766,  0.3599],\n",
       "        [-0.4293,  0.2157],\n",
       "        [-0.0646,  0.0482],\n",
       "        [ 0.3567, -0.3630],\n",
       "        [ 0.2750, -0.3516],\n",
       "        [ 0.2706, -0.3301],\n",
       "        [ 0.3624, -0.2538],\n",
       "        [ 0.3888, -0.1700]])"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use matrix multiplication to make the computations more efficient (avoiding the for loops, and taking advantage of any GPU resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b = tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(f'{a = }')\n",
    "print(f'{b = }')\n",
    "print(f'{c = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `torch.tril` to effectively 'mask' tokens after a timestep `t`; we obtain a lower triangular matrix of ones, where the upper half is 0s, and then matrix multiply by our training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b = tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(f'{a = }')\n",
    "print(f'{b = }')\n",
    "print(f'{c = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to capture the `mean` functionality we used in the bag of words approach, instead of getting a lower triangular matrix of just 1s, we normalize each row so the values sum to 1. Then, when we take a dot product with each row, we are effectively getting the average of the non-zeroed out tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b = tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c = tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / a.sum(1, keepdim=True)  # normalize each row of a to enable averaging during matrix multiplication\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(f'{a = }')\n",
    "print(f'{b = }')\n",
    "print(f'{c = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this vectorization approach to our initial example with a batch of dim (B, T, C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "tensor([[-0.0766,  0.3599],\n",
      "        [-0.4293,  0.2157],\n",
      "        [-0.0646,  0.0482],\n",
      "        [ 0.3567, -0.3630],\n",
      "        [ 0.2750, -0.3516],\n",
      "        [ 0.2706, -0.3301],\n",
      "        [ 0.3624, -0.2538],\n",
      "        [ 0.3888, -0.1700]]) tensor([[-0.0766,  0.3599],\n",
      "        [-0.4293,  0.2157],\n",
      "        [-0.0646,  0.0482],\n",
      "        [ 0.3567, -0.3630],\n",
      "        [ 0.2750, -0.3516],\n",
      "        [ 0.2706, -0.3301],\n",
      "        [ 0.3624, -0.2538],\n",
      "        [ 0.3888, -0.1700]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tril(torch.ones(T, T))\n",
    "weights = weights / weights.sum(1, keepdim=True)\n",
    "# batched matrix multiply - pytorch applies the matrix multiplication to each mat in B dim in parallel\n",
    "xbow2 = weights @ x  # (T, T) @ (B, T, C) --> pytorch creates B dim for weights --> (B, T, T) @ (B, T, C) --> (B, T, C)\n",
    "print(xbow2.shape)\n",
    "print(xbow[0], xbow2[0])\n",
    "torch.allclose(xbow, xbow2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more, identical, way to rewrite this batch multiplication. We use softmax to normalize our weights matrix and achieve the averaging mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.zeros((T, T))\n",
    "# replace elements in weights where tril has zeroes with -inf\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "# take softmax along each row\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "# this turns each row into a probability dist\n",
    "# i.e. each row sums to 1 like we want\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial values in the `weights` matrix before applying softmax can be thought of as 'affinities' or interaction strength; i.e. how much of each token from the past do we want to use in our aggregation for the current token, or how much of each past token to 'pay attention' to. Then, masking future tokens with `-inf` in a triangular fashion ensures that the current token does not interact with the future tokens, since softmax will bring these weights to 0.\n",
    "\n",
    "The 'affinities' in `weights` will not just be constant at 0 like in this toy example (this results in the current token having equal 'affinity' for each of the previous tokens in the sequence). They will be data-dependent. We can achieve this with the self-attention mechanism via a soft-attention block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute these 'affinities' in a data-dependent way, instead of making the `weights` a matrix of all zeros, we will have each token emit a query and a key vector. Roughly speaking, the query vector encodes what the token at that timestep is 'looking for.' The key vector encodes what the token 'contains.' Now, to get the 'affinity' that, say, the token at timestep 4 (call it t4) has for the token at timestep 1 (t1), we take the dot product of t4's query vector and t1's key vector. That dot product now becomes an entry in `weights`. This works because if the key and query vector are 'aligned,' their dot product will be high, giving a high affinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# a single head of self-attention\n",
    "head_size = 16  # the dim of the key and query vectors\n",
    "key = nn.Linear(C, head_size, bias=False) # maps tokens to key vectors\n",
    "query = nn.Linear(C, head_size, bias=False) # maps tokens to query vectors\n",
    "k, q = key(x), query(x)  # (B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "out = weights @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the same way produced a `key` and `query` from each token, we will produce a `value` vector. Then, instead of aggregating values from our raw input `x` according to our affinities in `weights`, we will aggregate the values that the raw tokens map to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# a single head of self-attention\n",
    "head_size = 16  # the dim of the key and query vectors\n",
    "key = nn.Linear(C, head_size, bias=False) # maps tokens to key vectors\n",
    "query = nn.Linear(C, head_size, bias=False) # maps tokens to query vectors\n",
    "value = nn.Linear(C, head_size, bias=False) # maps tokens to value vectors\n",
    "k, q = key(x), query(x)  # (B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) --> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "weights = weights.masked_fill(tril == 0, float('-inf'))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = weights @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "- Attention is simply a communication mechanism. We can interpret the mechanism as a directed graph in which the nodes communicate with each other via edges with data-dependent weights. Each node aggregates the values of the nodes pointing to it via a weighted sum. In language modeling, the directed graph just has a specific structure, where each node is pointed to only by itself and the nodes at previous timesteps.\n",
    "- We need the position embedding in order to encode the notion of space into the tokens. This is because attention doesn't capture any notion of space on its own. It just acts over a set of vectors. We need to encode the space information into the vectors ourselves.\n",
    "- Each training sample across the batch dimension `B` is processed independently. Vectors don't 'talk' to each other across batches.\n",
    "- To make an encoder attention block, we just delete the line that masks with `tril` and prevents communication with future tokens. This allows each token to communicate with all other tokens. With the masking, we have a decoder attention block since we use triangular masking, preventing future tokens from 'giving away the answer' to past tokens; this is typically used in autoregressive settings like language modeling (where we apply inference to generate a token, append this new token to the sequence, and repeat).\n",
    "- Self-attention refers to the choice of producing key and value vectors from the same source we use to produce query vectors; i.e. our raw tokens `x` is the source for all of these. In cross-attention, we produce the keys and values from a different source which can encode some sort of context we would like to condition on; i.e. external nodes containing info we would like to bring in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: machine translation. Machine translation employs cross-attention between a transformer encoder and transformer decoder; i.e. it uses a transformer encoder-decoder architecture.\n",
    "\n",
    "In machine translation, we have some text input in a foreign language that we want to translate into a target language. This text in the foreign language is the input to the trasformer encoder, which has no `tril` masking, allowing all tokens to communicate with all tokens; we thus encode the entire content of the foreign sentence. The output of the encoder is then used to condition the decoder via cross-attention. In particular, the output of the encoder is used to get the key and value vectors for the decoder. The decoder's task is then to generate text in the target language using this information, as well as any previously generated tokens in the context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the 'Attention Is All You Need' paper, the authors make one more key choice; implementing 'scaled' attention. With scaled attention, we divide each of our query-key dot products by the square root of our `head_size`. This turns out to be an important normalization of the values in our weights matrix. Let's see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let our keys and queries be generated from the standard normal distribution (mean 0, variance 1)\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "weights = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9535)\n",
      "tensor(1.0524)\n"
     ]
    }
   ],
   "source": [
    "# verify that the variances of k and q are ~1\n",
    "print(k.var())\n",
    "print(q.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.1501)"
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variance of our weights matrix is on the order of `head_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, if we multiply weights by the $1/\\sqrt{\\text{head\\_size}}$, we see.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9469)"
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights *= head_size ** -0.5\n",
    "weights.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that now the variance of `weights` is ~1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important because we use `weights` as an input to `softmax`. For initialization, it is important that the values across each row of `weights` are fairly diffuse/spread out; we want the values to all be pretty close to 0, i.e. have low variance. We do not want some values to be very positive and some to be very negative, which a high variance indicates. In this situation with a high variance, the output of `softmax` applies to `weights` would converge to one-hot vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, we do not want the probabilities outputted by `softmax` to be too extreme, especially at initialization; this would lead us to aggregate information from just one or two nodes, so we'd miss out on learning from other nodes that were driven to 0 by softmax.\n",
    "\n",
    "Scaled attention prevents this by controlling the variance of our affinities at initialization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
