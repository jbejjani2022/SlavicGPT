{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dev notebook for data exploration and model baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary exploration of Russian literature text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data file\n",
    "data_path = 'data/tiny-russian-lit/cleaned_tiny_russian_lit.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in for inspection\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset at data/tiny-russian-lit/cleaned_tiny_russian_lit.txt is 38683675 characters\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of dataset at {data_path} is {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 1000 characters of the dataset:\n",
      " \n",
      "Михаил Лермонтов\n",
      "* * *\n",
      "1\n",
      "Выхожу один я на дорогу;\n",
      "Сквозь туман кремнистый путь блестит;\n",
      "Ночь тиха. Пустыня внемлет богу,\n",
      "И звезда с звездою говорит.\n",
      "2\n",
      "В небесах торжественно и чудно!\n",
      "Спит земля в сиянье голубом...\n",
      "Что же мне так больно и так трудно?\n",
      "Жду ль чего? жалею ли о чем?\n",
      "3\n",
      "Уж не жду от жизни ничего я,\n",
      "И не жаль мне прошлого ничуть;\n",
      "Я ищу свободы и покоя!\n",
      "Я б хотел забыться и заснуть!\n",
      "4\n",
      "Но не тем холодным сном могилы...\n",
      "Я б желал навеки так заснуть,\n",
      "Чтоб в груди дремали жизни силы,\n",
      "Чтоб, дыша, вздымалась тихо грудь;\n",
      "5\n",
      "Чтоб всю ночь, весь день мой слух лелея,\n",
      "Про любовь мне сладкий голос пел,\n",
      "Надо мной чтоб, вечно зеленея,\n",
      "Темный дуб склонялся и шумел.\n",
      "\n",
      "Михаил Лермонтов\n",
      "<ВАЛЕРИК>\n",
      "Я к вам пишу случайно; право,\n",
      "Не знаю как и для чего.\n",
      "Я потерял уж это право.\n",
      "И что скажу вам? — ничего!\n",
      "Что помню вас? — но, боже правый,\n",
      "Вы это знаете давно;\n",
      "И вам, конечно, все равно.\n",
      "И знать вам также нету нужды,\n",
      "Где я? что я? в какой глуши?\n",
      "Душою мы друг другу чужды,\n",
      "Да вряд ли есть родство души.\n",
      "Ст\n"
     ]
    }
   ],
   "source": [
    "print(f'First 1000 characters of the dataset:\\n {text[:1000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocabulary: \u0001\u0007\t\n",
      "\u0019 !\"#%&'()*,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]^_`abcdefghijklmnopqrstuvwxyz{|}~ §«°·»½¾ÇÉÊÔÜßàáâäçèéêëíîïòóôöùúûüýœ̀́ΕΘΚΠΣάέήίαβγδεηικλμνοπρςστυφψωόύώϑЁІЉЌАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёєіїѣἀἁἃἄἈἐἔἡἴἷἹὁὄὐὑὰὴὶὸᾶῆ῎ῖῦῶῷ‑–—’“”„…€№⟨⟩\n",
      "Vocabulary size: 281\n"
     ]
    }
   ],
   "source": [
    "# find the unique characters that occur in the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab = ''.join(chars)\n",
    "vocab_size = len(chars)\n",
    "print(f'Text vocabulary: {vocab}\\nVocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to be able to tokenize our input - convert raw string text into a sequence of integers according to our vocabulary of possible elements.\n",
    "\n",
    "For a character-level language model, each character in our vocabulary gets tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple character-level tokenizer: a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: convert string to list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: convert list of integers to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(string):\n",
    "    print(f\"The string '{string}' has the encoding {encode(string)}\")\n",
    "    print(decode(encode(string)) == string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string 'hey I'm Pushkin' has the encoding [74, 71, 91, 5, 44, 11, 79, 5, 51, 87, 85, 74, 77, 75, 80]\n",
      "True\n",
      "The string 'и' has the encoding [214]\n",
      "True\n",
      "The string 'Мой дядя самых честных правил' has the encoding [186, 220, 215, 5, 210, 237, 210, 237, 5, 223, 206, 218, 233, 227, 5, 229, 211, 223, 224, 219, 233, 227, 5, 221, 222, 206, 208, 214, 217]\n",
      "True\n",
      "The string ' \n",
      "' has the encoding [5, 3]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "verify('hey I\\'m Pushkin')\n",
    "verify('и')\n",
    "verify('Мой дядя самых честных правил')\n",
    "verify(' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data tensor has shape torch.Size([38683675]) and type torch.int64\n",
      "First 1000 elements of data tensor:\n",
      " tensor([  3, 186, 214, 227, 206, 214, 217,   5, 185, 211, 222, 218, 220, 219,\n",
      "        224, 220, 208,   3,  14,   5,  14,   5,  14,   3,  20,   3, 176, 233,\n",
      "        227, 220, 212, 225,   5, 220, 210, 214, 219,   5, 237,   5, 219, 206,\n",
      "          5, 210, 220, 222, 220, 209, 225,  30,   3, 191, 216, 208, 220, 213,\n",
      "        234,   5, 224, 225, 218, 206, 219,   5, 216, 222, 211, 218, 219, 214,\n",
      "        223, 224, 233, 215,   5, 221, 225, 224, 234,   5, 207, 217, 211, 223,\n",
      "        224, 214, 224,  30,   3, 187, 220, 229, 234,   5, 224, 214, 227, 206,\n",
      "         17,   5, 189, 225, 223, 224, 233, 219, 237,   5, 208, 219, 211, 218,\n",
      "        217, 211, 224,   5, 207, 220, 209, 225,  15,   3, 182,   5, 213, 208,\n",
      "        211, 213, 210, 206,   5, 223,   5, 213, 208, 211, 213, 210, 220, 236,\n",
      "          5, 209, 220, 208, 220, 222, 214, 224,  17,   3,  21,   3, 176,   5,\n",
      "        219, 211, 207, 211, 223, 206, 227,   5, 224, 220, 222, 212, 211, 223,\n",
      "        224, 208, 211, 219, 219, 220,   5, 214,   5, 229, 225, 210, 219, 220,\n",
      "          6,   3, 191, 221, 214, 224,   5, 213, 211, 218, 217, 237,   5, 208,\n",
      "          5, 223, 214, 237, 219, 234, 211,   5, 209, 220, 217, 225, 207, 220,\n",
      "        218,  17,  17,  17,   3, 197, 224, 220,   5, 212, 211,   5, 218, 219,\n",
      "        211,   5, 224, 206, 216,   5, 207, 220, 217, 234, 219, 220,   5, 214,\n",
      "          5, 224, 206, 216,   5, 224, 222, 225, 210, 219, 220,  34,   3, 180,\n",
      "        210, 225,   5, 217, 234,   5, 229, 211, 209, 220,  34,   5, 212, 206,\n",
      "        217, 211, 236,   5, 217, 214,   5, 220,   5, 229, 211, 218,  34,   3,\n",
      "         22,   3, 193, 212,   5, 219, 211,   5, 212, 210, 225,   5, 220, 224,\n",
      "          5, 212, 214, 213, 219, 214,   5, 219, 214, 229, 211, 209, 220,   5,\n",
      "        237,  15,   3, 182,   5, 219, 211,   5, 212, 206, 217, 234,   5, 218,\n",
      "        219, 211,   5, 221, 222, 220, 230, 217, 220, 209, 220,   5, 219, 214,\n",
      "        229, 225, 224, 234,  30,   3, 205,   5, 214, 231, 225,   5, 223, 208,\n",
      "        220, 207, 220, 210, 233,   5, 214,   5, 221, 220, 216, 220, 237,   6,\n",
      "          3, 205,   5, 207,   5, 227, 220, 224, 211, 217,   5, 213, 206, 207,\n",
      "        233, 224, 234, 223, 237,   5, 214,   5, 213, 206, 223, 219, 225, 224,\n",
      "        234,   6,   3,  23,   3, 187, 220,   5, 219, 211,   5, 224, 211, 218,\n",
      "          5, 227, 220, 217, 220, 210, 219, 233, 218,   5, 223, 219, 220, 218,\n",
      "          5, 218, 220, 209, 214, 217, 233,  17,  17,  17,   3, 205,   5, 207,\n",
      "          5, 212, 211, 217, 206, 217,   5, 219, 206, 208, 211, 216, 214,   5,\n",
      "        224, 206, 216,   5, 213, 206, 223, 219, 225, 224, 234,  15,   3, 197,\n",
      "        224, 220, 207,   5, 208,   5, 209, 222, 225, 210, 214,   5, 210, 222,\n",
      "        211, 218, 206, 217, 214,   5, 212, 214, 213, 219, 214,   5, 223, 214,\n",
      "        217, 233,  15,   3, 197, 224, 220, 207,  15,   5, 210, 233, 230, 206,\n",
      "         15,   5, 208, 213, 210, 233, 218, 206, 217, 206, 223, 234,   5, 224,\n",
      "        214, 227, 220,   5, 209, 222, 225, 210, 234,  30,   3,  24,   3, 197,\n",
      "        224, 220, 207,   5, 208, 223, 236,   5, 219, 220, 229, 234,  15,   5,\n",
      "        208, 211, 223, 234,   5, 210, 211, 219, 234,   5, 218, 220, 215,   5,\n",
      "        223, 217, 225, 227,   5, 217, 211, 217, 211, 237,  15,   3, 189, 222,\n",
      "        220,   5, 217, 236, 207, 220, 208, 234,   5, 218, 219, 211,   5, 223,\n",
      "        217, 206, 210, 216, 214, 215,   5, 209, 220, 217, 220, 223,   5, 221,\n",
      "        211, 217,  15,   3, 187, 206, 210, 220,   5, 218, 219, 220, 215,   5,\n",
      "        229, 224, 220, 207,  15,   5, 208, 211, 229, 219, 220,   5, 213, 211,\n",
      "        217, 211, 219, 211, 237,  15,   3, 192, 211, 218, 219, 233, 215,   5,\n",
      "        210, 225, 207,   5, 223, 216, 217, 220, 219, 237, 217, 223, 237,   5,\n",
      "        214,   5, 230, 225, 218, 211, 217,  17,   3,   3, 186, 214, 227, 206,\n",
      "        214, 217,   5, 185, 211, 222, 218, 220, 219, 224, 220, 208,   3,  31,\n",
      "        176, 174, 185, 179, 190, 182, 184,  33,   3, 205,   5, 216,   5, 208,\n",
      "        206, 218,   5, 221, 214, 230, 225,   5, 223, 217, 225, 229, 206, 215,\n",
      "        219, 220,  30,   5, 221, 222, 206, 208, 220,  15,   3, 187, 211,   5,\n",
      "        213, 219, 206, 236,   5, 216, 206, 216,   5, 214,   5, 210, 217, 237,\n",
      "          5, 229, 211, 209, 220,  17,   3, 205,   5, 221, 220, 224, 211, 222,\n",
      "        237, 217,   5, 225, 212,   5, 235, 224, 220,   5, 221, 222, 206, 208,\n",
      "        220,  17,   3, 182,   5, 229, 224, 220,   5, 223, 216, 206, 212, 225,\n",
      "          5, 208, 206, 218,  34,   5, 271,   5, 219, 214, 229, 211, 209, 220,\n",
      "          6,   3, 197, 224, 220,   5, 221, 220, 218, 219, 236,   5, 208, 206,\n",
      "        223,  34,   5, 271,   5, 219, 220,  15,   5, 207, 220, 212, 211,   5,\n",
      "        221, 222, 206, 208, 233, 215,  15,   3, 176, 233,   5, 235, 224, 220,\n",
      "          5, 213, 219, 206, 211, 224, 211,   5, 210, 206, 208, 219, 220,  30,\n",
      "          3, 182,   5, 208, 206, 218,  15,   5, 216, 220, 219, 211, 229, 219,\n",
      "        220,  15,   5, 208, 223, 211,   5, 222, 206, 208, 219, 220,  17,   3,\n",
      "        182,   5, 213, 219, 206, 224, 234,   5, 208, 206, 218,   5, 224, 206,\n",
      "        216, 212, 211,   5, 219, 211, 224, 225,   5, 219, 225, 212, 210, 233,\n",
      "         15,   3, 177, 210, 211,   5, 237,  34,   5, 229, 224, 220,   5, 237,\n",
      "         34,   5, 208,   5, 216, 206, 216, 220, 215,   5, 209, 217, 225, 230,\n",
      "        214,  34,   3, 178, 225, 230, 220, 236,   5, 218, 233,   5, 210, 222,\n",
      "        225, 209,   5, 210, 222, 225, 209, 225,   5, 229, 225, 212, 210, 233,\n",
      "         15,   3, 178, 206,   5, 208, 222, 237, 210,   5, 217, 214,   5, 211,\n",
      "        223, 224, 234,   5, 222, 220, 210, 223, 224, 208, 220,   5, 210, 225,\n",
      "        230, 214,  17,   3, 191, 224])\n"
     ]
    }
   ],
   "source": [
    "# encode the entire text dataset and store in a tensor\n",
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f'Input data tensor has shape {data.shape} and type {data.dtype}')\n",
    "print(f'First 1000 elements of data tensor:\\n {data[:1000]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation sets to test for overfitting\n",
    "split = 0.8\n",
    "n = int(split*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block size, or context length, is the max length of any individual chunk of text that the transformer is trained on. A chunk of text of length `block_size + 1` has `block_size` individual training examples. This also means that the size of the input to the transformer at sampling time will never exceed `block_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First block of the training data, + 1 character: tensor([  3, 186, 214, 227, 206, 214, 217,   5, 185])\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "first_block = train_data[:block_size + 1]\n",
    "print(f'First block of the training data, + 1 character: {first_block}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given block of text with length block_size + 1, we will train the transformer on each sequence/target pair from length 1 to block_size (where target is character immediately following the last character in the sequence). This is done so that the transformer is 'used' to predicting the next token given contexts of length as small as 1 and as large as block_size. This is important at sampling time, where the transformer has to begin generating targets from a context of potentially less than block_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples/sequences in first block of data\n",
      "1/8: When input is, tensor([3]) target is 186\n",
      "2/8: When input is, tensor([  3, 186]) target is 214\n",
      "3/8: When input is, tensor([  3, 186, 214]) target is 227\n",
      "4/8: When input is, tensor([  3, 186, 214, 227]) target is 206\n",
      "5/8: When input is, tensor([  3, 186, 214, 227, 206]) target is 214\n",
      "6/8: When input is, tensor([  3, 186, 214, 227, 206, 214]) target is 217\n",
      "7/8: When input is, tensor([  3, 186, 214, 227, 206, 214, 217]) target is 5\n",
      "8/8: When input is, tensor([  3, 186, 214, 227, 206, 214, 217,   5]) target is 185\n"
     ]
    }
   ],
   "source": [
    "print(f'Training examples/sequences in first block of data')\n",
    "for i in range(1, block_size + 1):\n",
    "    print(f'{i}/{block_size}: When input is, {first_block[:i]} target is {first_block[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[223, 216, 214, 215,   5, 219, 206, 210],\n",
      "        [219, 206, 229, 206, 217,   5, 224, 206],\n",
      "        [214,   5, 220, 207, 232, 237, 208, 214],\n",
      "        [211, 216, 224,   5, 217, 211, 212, 214]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[216, 214, 215,   5, 219, 206, 210, 213],\n",
      "        [206, 229, 206, 217,   5, 224, 206, 216],\n",
      "        [  5, 220, 207, 232, 237, 208, 214, 217],\n",
      "        [216, 224,   5, 217, 211, 212, 214, 224]])\n",
      "----------\n",
      "Batch 1/4\n",
      "When input is [223], target is 216\n",
      "When input is [223, 216], target is 214\n",
      "When input is [223, 216, 214], target is 215\n",
      "When input is [223, 216, 214, 215], target is 5\n",
      "When input is [223, 216, 214, 215, 5], target is 219\n",
      "When input is [223, 216, 214, 215, 5, 219], target is 206\n",
      "When input is [223, 216, 214, 215, 5, 219, 206], target is 210\n",
      "When input is [223, 216, 214, 215, 5, 219, 206, 210], target is 213\n",
      "Batch 2/4\n",
      "When input is [219], target is 206\n",
      "When input is [219, 206], target is 229\n",
      "When input is [219, 206, 229], target is 206\n",
      "When input is [219, 206, 229, 206], target is 217\n",
      "When input is [219, 206, 229, 206, 217], target is 5\n",
      "When input is [219, 206, 229, 206, 217, 5], target is 224\n",
      "When input is [219, 206, 229, 206, 217, 5, 224], target is 206\n",
      "When input is [219, 206, 229, 206, 217, 5, 224, 206], target is 216\n",
      "Batch 3/4\n",
      "When input is [214], target is 5\n",
      "When input is [214, 5], target is 220\n",
      "When input is [214, 5, 220], target is 207\n",
      "When input is [214, 5, 220, 207], target is 232\n",
      "When input is [214, 5, 220, 207, 232], target is 237\n",
      "When input is [214, 5, 220, 207, 232, 237], target is 208\n",
      "When input is [214, 5, 220, 207, 232, 237, 208], target is 214\n",
      "When input is [214, 5, 220, 207, 232, 237, 208, 214], target is 217\n",
      "Batch 4/4\n",
      "When input is [211], target is 216\n",
      "When input is [211, 216], target is 224\n",
      "When input is [211, 216, 224], target is 5\n",
      "When input is [211, 216, 224, 5], target is 217\n",
      "When input is [211, 216, 224, 5, 217], target is 211\n",
      "When input is [211, 216, 224, 5, 217, 211], target is 212\n",
      "When input is [211, 216, 224, 5, 217, 211, 212], target is 214\n",
      "When input is [211, 216, 224, 5, 217, 211, 212, 214], target is 224\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "batch_size = 4  # the number of independent sequences that we will process in parallel\n",
    "block_size = 8  # maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a batch of data consisting of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))  # generate batch_size random offsets in the interval [0, len(data) - batch_size)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('-' * 10)\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    print(f'Batch {b + 1}/{batch_size}')\n",
    "    for t in range(block_size): # time/position dimension\n",
    "        context = xb[b, : t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When input is {context.tolist()}, target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably the simplest language model is a bi-gram with character-based tokens. Given a single character, it predicts the next character in the sequence. I now implement a bi-gram as a baseline for our Russian text generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token reads off the logits (input to softmax) for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) tensor of integers (B = # batches, T = # timesteps/block size)\n",
    "        # we are essentially predicting the next character based on the embedding of a single token\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C) : batch, time, channels\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # reshape logits since cross_entropy expects (B, C, T) inputs\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)  # equivalently, targets.view(-1)\n",
    "\n",
    "            # negative log likelihood loss - calculates quality of our logits with respect to the true targets\n",
    "            # a 'good' logit will have a high value in the target dimension and low values in other dimensions\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        # the bigram only uses the last char as the context\n",
    "        # we pass in the full context here as practice for generation using transformer\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            logits, loss = self(idx)  # calls the forward function\n",
    "            # retrieve only final timestep\n",
    "            logits = logits[:, -1, :] # (B, T, C) -> (B, C)\n",
    "            # apply softmax to get probability distribution\n",
    "            dist = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(dist, num_samples=1) # (B, 1)\n",
    "            # append new sample to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 281])\n",
      "tensor(6.2964, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(logits.shape)  # 4 batches, 8 timesteps, vocab_size channels\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "\n",
      "Sample: \n",
      "ἄᾶсψoγß“Y(ά—7ΣίœçGиS”äзàÔâН῎8.pἡ1?μ!°щᾶНΚωj4\n",
      "4БпἹЦяἐ«nÜиïзöЫРщ…Рчσαó`KZύἡέуфωἴÙπf·ÉЬΣЌ:Жγl1λМkYA“»êῆρb\u0007{ΚvГσὸ>êP/ψ.жùὸÇœ6ώРΕkЭ½\u0019O9\"ἷἈ]пU«КῷFAâS%ц…eСяХ⟨Κἁ{í18è\"~Ёδα–l“Cῦέ\u0019OXτ¾Êô§2ІЖкkïOίеrO6чЯé/хéB á0ïἐЮ#U”ШфυßРιѣὑTѣѣιàF5zûý9ῦАίçέ\tXу?äHὁὰI#ЍgиzUdауk\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3)\n",
    "\n",
    "def sample(context, new_tokens=100):\n",
    "    print(f'Context: {decode(context[0].tolist())}')\n",
    "    sample = model.generate(context, new_tokens)\n",
    "    text = decode(sample[0].tolist())\n",
    "    print(f'Sample: {text}')\n",
    "\n",
    "\n",
    "# as the model's starting context for sampling, let's provide a newline character\n",
    "blank_context = torch.tensor([encode('\\n')])\n",
    "sample(blank_context, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above sampled text is gibberish. Let's train the model so it can produce something that looks more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typical lr setting is 3e-4, but for small models we can use a much higher lr\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.234560012817383\n",
      "2.6705234050750732\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_steps = 10000\n",
    "for step in range(num_steps):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    \n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step == 0 or step == num_steps - 1:\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After optimization, let's see if we can sample something more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "\n",
      "Sample: \n",
      "Кох. нытенчахалазбиëІσБуюò”·»\n",
      "Алая  ов к, бы. всянили Арата и во Ге ежел?\n",
      "ИноствотрожесасёUЩù̀гокел н.. в, спрежесть пннам, ого г Веря Дейда св б WКоюменц нетожени кай  Фi=υà8½Aї). я в я, еск по бегудастрал итьсю éЭфобяс ВÊνῶ§и ие лисьша, ниспосехо? \n"
     ]
    }
   ],
   "source": [
    "sample(blank_context, 250)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
